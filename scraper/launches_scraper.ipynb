{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_launch_data(url):\n",
    "    # Fetch the content of the URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize dictionary to store the scraped data\n",
    "    data = {}\n",
    "\n",
    "    # Extract title, launch time, descriptions, and video/article\n",
    "    section = soup.find(\"section\", class_=\"card section--center white mdl-grid mdl-grid--no-spacing mdl-shadow--6dp\")\n",
    "\n",
    "    if section:\n",
    "        # Extract status\n",
    "        status_tag = section.find(\"h6\", class_=\"status\")\n",
    "        data[\"mission_status\"] = status_tag.text.strip() if status_tag else None\n",
    "        \n",
    "        # Extract title\n",
    "        title_tag = section.find(\"h4\", class_=\"mdl-card__title-text\")\n",
    "        data[\"title\"] = title_tag.text.strip() if title_tag else None\n",
    "        \n",
    "        # Extract launch time from the main tag\n",
    "        launch_time_tag = section.find(\"span\", id=\"localized\")\n",
    "        if launch_time_tag:\n",
    "            data[\"launch_time\"] = launch_time_tag.text.strip()\n",
    "        else:\n",
    "            # Directly search for the strong tag within the section\n",
    "            strong_tag = section.find(\"strong\", string=\"Launch Time\")\n",
    "            if strong_tag:\n",
    "                # Get the parent div and extract all text\n",
    "                launch_time_cell = strong_tag.find_parent(\"div\")\n",
    "                if launch_time_cell:\n",
    "                    # Use stripped_strings to get clean text\n",
    "                    launch_time_text = ''.join(launch_time_cell.stripped_strings)\n",
    "                    # Remove the \"Launch Time\" label from the text\n",
    "                    data[\"launch_time\"] = launch_time_text.replace(\"Launch Time\", \"\").strip()\n",
    "                else:\n",
    "                    data[\"launch_time\"] = None\n",
    "            else:\n",
    "                data[\"launch_time\"] = None\n",
    "\n",
    "        \n",
    "        # Extract description\n",
    "        description_tag = section.find(\"div\", class_=\"mdl-card__supporting-text\").find(\"p\")\n",
    "        data[\"description\"] = description_tag.text.strip() if description_tag else None\n",
    "\n",
    "        # Extract video links\n",
    "        video_select = section.find(\"select\", id=\"videos\")\n",
    "        if video_select:\n",
    "            data[\"videos\"] = {opt.text.strip(): opt.get(\"data-url\") for opt in video_select.find_all(\"option\") if opt.get(\"data-url\")}\n",
    "        \n",
    "        # Extract article link\n",
    "        links = section.find_all(\"a\")\n",
    "        data[\"article_link\"] = None\n",
    "        for link in links:\n",
    "            if \"watch\" not in link.text.strip().lower():\n",
    "                data[\"article_link\"] = link[\"href\"]\n",
    "\n",
    "    # Extract Rocket Information\n",
    "    rocket_data = {}\n",
    "    rocket_section = soup.find(\"section\", class_=\"card section--center mdl-grid mdl-grid--no-spacing mdl-shadow--6dp\")\n",
    "    if rocket_section:\n",
    "        # Rocket Name\n",
    "        header_section = rocket_section.find(\"header\", class_=\"image_header\")\n",
    "        rocket_name_div = header_section.find(\"div\", class_=\"mdl-card__title-text\")\n",
    "        rocket_data[\"rocket_name\"] = rocket_name_div.find(\"span\").text.strip() if rocket_name_div else None\n",
    "\n",
    "        # Find the second <style> tag in the parent div\n",
    "        style_tags = soup.find(\"div\", class_=\"page-content\").find_all(\"style\")\n",
    "        if len(style_tags) > 1:\n",
    "            second_style = style_tags[1].string\n",
    "            # Extract the image URL from the second style section\n",
    "            if \"rocket_image\" in second_style:\n",
    "                match = re.search(r'background:\\s*url\\((.*?)\\)', second_style)\n",
    "                if match:\n",
    "                    rocket_data[\"image_url\"] = match.group(1).strip(' \"\\'')\n",
    "                else:\n",
    "                    rocket_data[\"image_url\"] = None\n",
    "            else:\n",
    "                rocket_data[\"image_url\"] = None\n",
    "        else:\n",
    "            rocket_data[\"image_url\"] = None\n",
    "\n",
    "        # Image Credit\n",
    "        image_credit_div = header_section.find(\"div\", class_=\"right_justified_text\")\n",
    "        rocket_data[\"image_credit\"] = image_credit_div.find(\"span\").text.strip() if image_credit_div else None\n",
    "\n",
    "        # Rocket Details\n",
    "        rocket_details = rocket_section.find(\"div\", class_=\"mdl-card__supporting-text\")\n",
    "        if rocket_details:\n",
    "            rocket_details_info = rocket_details.find_all(\"div\", class_=\"mdl-cell mdl-cell--6-col-desktop mdl-cell--12-col-tablet\")\n",
    "            for detail in rocket_details_info:\n",
    "                key_value = detail.text.split(\":\")\n",
    "                if len(key_value) == 2:\n",
    "                    key = key_value[0].strip().replace(\" \", \"_\").lower()\n",
    "                    value = key_value[1].strip()\n",
    "                    rocket_data[key] = value\n",
    "\n",
    "        # Rocket Details Link\n",
    "        details_link_tag = rocket_section.find(\"div\", class_=\"mdl-card__actions mdl-card--border\").find(\"a\", class_=\"mdc-button\")\n",
    "        rocket_data[\"rocket_details_link\"] = details_link_tag.get(\"href\") if details_link_tag and details_link_tag.get(\"href\") else None\n",
    "\n",
    "        # Get Rocket ID from the link\n",
    "        if rocket_data[\"rocket_details_link\"]:\n",
    "            rocket_data[\"rocket_id\"] = rocket_data[\"rocket_details_link\"].split(\"/\")[-1]\n",
    "\n",
    "    # Store rocket data in the main data dictionary\n",
    "    data[\"rocket_data\"] = rocket_data\n",
    "\n",
    "    # Extract Mission Details\n",
    "    mission_data = {}\n",
    "    mission_section = soup.find(\"section\", class_=\"section--center card white mdl-grid mdl-grid--no-spacing mdl-shadow--6dp\")\n",
    "    if mission_section:\n",
    "        # Mission Title\n",
    "        mission_title = mission_section.find(\"h4\", class_=\"mdl-card__title-text\")\n",
    "        mission_data[\"title\"] = mission_title.text.strip() if mission_title else None\n",
    "\n",
    "        # Mission Description\n",
    "        description_paragraph = mission_section.find(\"div\", class_=\"mdl-grid a\").find(\"p\")\n",
    "        mission_data[\"description\"] = description_paragraph.text.strip() if description_paragraph else None\n",
    "\n",
    "        # Mission Details\n",
    "        details_grids = mission_section.find_all(\"div\", class_=\"mdl-grid a\")\n",
    "        if len(details_grids) > 1:  # Check if there are at least two grids\n",
    "            details_info = details_grids[1].find_all(\"div\", class_=\"mdl-cell\")\n",
    "            for detail in details_info:\n",
    "                text = detail.text.strip()\n",
    "                if \"Payloads:\" in text:\n",
    "                    mission_data[\"payloads\"] = text.split(\":\")[1].strip()\n",
    "                elif \"Total Mass:\" in text:\n",
    "                    mission_data[\"total_mass\"] = text.split(\":\")[1].strip()\n",
    "                elif \"Orbit\" in text:\n",
    "                    mission_data[\"orbit\"] = text\n",
    "\n",
    "    # Store mission data in the main data dictionary\n",
    "    data[\"mission_data\"] = mission_data\n",
    "\n",
    "    # Location Information\n",
    "    location_section = soup.find(\"h3\", string=\"Location\").find_next(\"section\", class_=\"card\")\n",
    "    if location_section:\n",
    "        location_title = location_section.find(\"h4\", class_=\"mdl-card__title-text\")\n",
    "        data[\"location\"] = location_title.text.strip() if location_title else None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL configurations for past and upcoming launches\n",
    "configurations = {\n",
    "    \"past\": {\"base_url\": \"https://nextspaceflight.com/launches/past/\", \"pages\": 229, \"output\": \"past_launches.json\"},\n",
    "    \"upcoming\": {\"base_url\": \"https://nextspaceflight.com/launches/\", \"pages\": 15, \"output\": \"upcoming_launches.json\"}\n",
    "}\n",
    "\n",
    "# Iterate over configurations to scrape both past and upcoming launches\n",
    "for config_name, config in configurations.items():\n",
    "    print(f\"Scraping {config_name} launches...\")\n",
    "\n",
    "    # Create URL array and collect detail URLs\n",
    "    url_array = create_url_array(config[\"base_url\"], config[\"pages\"])\n",
    "    detail_urls = collect_detail_urls(url_array)\n",
    "\n",
    "    # Scrape launch data and collect failed URLs\n",
    "    launch_data, total_success, total_failures, failed_urls = scrape_with_counts(detail_urls, scrape_launch_data)\n",
    "\n",
    "    # Save the scraped data to a JSON file\n",
    "    with open(config[\"output\"], 'w') as json_file:\n",
    "        json.dump(launch_data, json_file, indent=4)\n",
    "    \n",
    "    print(f\"{config_name.capitalize()} launch scraping complete.\")\n",
    "    print(f\"Total Success: {total_success}, Total Failures: {total_failures}. Data saved to '{config['output']}'.\")\n",
    "    if failed_urls:\n",
    "        print(f\"Failed URLs: {failed_urls}\")\n",
    "\n",
    "print(\"All scraping operations are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Function to make a request with retry mechanism and timeout\n",
    "def fetch_url(url, max_retries=3, timeout=10):\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            attempts += 1\n",
    "            print(f\"Attempt {attempts}/{max_retries} failed for {url}: {e}\")\n",
    "            if attempts < max_retries:\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "            else:\n",
    "                print(f\"Giving up on {url} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# Main scraping loop with enhanced error handling and retry mechanism\n",
    "url_array = ['https://nextspaceflight.com/launches/past/?search=']\n",
    "\n",
    "# Add more pages to the array\n",
    "for i in range(2, 3):\n",
    "    url_string = f\"https://nextspaceflight.com/launches/past/?page={i}&search=\"\n",
    "    url_array.append(url_string)\n",
    "\n",
    "# Array to hold detail page URLs\n",
    "detail_array = []\n",
    "\n",
    "# Loop through URLs and get detail page URLs\n",
    "for i in url_array:\n",
    "    page_content = fetch_url(i)\n",
    "    if page_content:\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        links = soup.find_all(class_='mdc-button')\n",
    "        for j in links:\n",
    "            if j['href'][0] == '/':\n",
    "                detail_url = f\"https://nextspaceflight.com{j['href']}\"\n",
    "                detail_array.append(detail_url)\n",
    "\n",
    "# List to hold launch data\n",
    "launches_scrape = []\n",
    "counter = 0\n",
    "\n",
    "# Loop through detail URLs and extract information using the function\n",
    "for detail_url in detail_array:\n",
    "    try:\n",
    "        # Call the function to scrape each detail page\n",
    "        launch_data = scrape_launch_data(detail_url)\n",
    "        if launch_data:\n",
    "            launches_scrape.append(launch_data)\n",
    "            print(f\"Scraped {detail_url}\")\n",
    "        else:\n",
    "            print(f\"Failed to scrape {detail_url}.\")\n",
    "        \n",
    "        counter += 1\n",
    "        print(f\"Total launches scraped: {counter}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error scraping {detail_url}: {e}\")\n",
    "\n",
    "# Write the scraped data to a JSON file\n",
    "output_json_path = 'launches.json'\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(launches_scrape, json_file, indent=4)\n",
    "\n",
    "print(f\"Scraping complete. Data saved to '{output_json_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "# Function to create an array of URLs for multiple pages\n",
    "def create_url_array(base_url, pages):\n",
    "    url_array = [base_url]\n",
    "    for i in range(2, pages + 1):\n",
    "        url_string = f\"{base_url}?page={i}\"\n",
    "        url_array.append(url_string)\n",
    "    return url_array\n",
    "\n",
    "# Function to fetch URL content\n",
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to collect detail page URLs from the base URLs\n",
    "def collect_detail_urls(url_array):\n",
    "    detail_urls = []\n",
    "    for url in url_array:\n",
    "        page_content = fetch_url(url)\n",
    "        if page_content:\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            links = soup.find_all(class_='mdc-button')\n",
    "            for link in links:\n",
    "                if link['href'].startswith('/'):\n",
    "                    detail_urls.append(f\"https://nextspaceflight.com{link['href']}\")\n",
    "    return detail_urls\n",
    "\n",
    "# Function to scrape launch data from a detail page\n",
    "def scrape_launch_data(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    section = soup.find(\"section\", class_=\"card section--center white mdl-grid mdl-grid--no-spacing mdl-shadow--6dp\")\n",
    "\n",
    "    if section:\n",
    "        # Extract status\n",
    "        status_tag = section.find(\"h6\", class_=\"status\")\n",
    "        data[\"mission_status\"] = status_tag.text.strip() if status_tag else None\n",
    "\n",
    "        # Extract title\n",
    "        title_tag = section.find(\"h4\", class_=\"mdl-card__title-text\")\n",
    "        data[\"title\"] = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        # Extract launch time\n",
    "        launch_time_tag = section.find(\"span\", id=\"localized\")\n",
    "        if launch_time_tag:\n",
    "            data[\"launch_time\"] = launch_time_tag.text.strip()\n",
    "        else:\n",
    "            strong_tag = section.find(\"strong\", string=\"Launch Time\")\n",
    "            if strong_tag:\n",
    "                launch_time_cell = strong_tag.find_parent(\"div\")\n",
    "                if launch_time_cell:\n",
    "                    launch_time_text = ''.join(launch_time_cell.stripped_strings)\n",
    "                    data[\"launch_time\"] = launch_time_text.replace(\"Launch Time\", \"\").strip()\n",
    "\n",
    "        # Extract description\n",
    "        description_tag = section.find(\"div\", class_=\"mdl-card__supporting-text\").find(\"p\")\n",
    "        data[\"description\"] = description_tag.text.strip() if description_tag else None\n",
    "\n",
    "        # Extract video links\n",
    "        video_select = section.find(\"select\", id=\"videos\")\n",
    "        if video_select:\n",
    "            data[\"videos\"] = {opt.text.strip(): opt.get(\"data-url\") for opt in video_select.find_all(\"option\") if opt.get(\"data-url\")}\n",
    "\n",
    "        # Extract article link\n",
    "        links = section.find_all(\"a\")\n",
    "        data[\"article_link\"] = None\n",
    "        for link in links:\n",
    "            if \"watch\" not in link.text.strip().lower():\n",
    "                data[\"article_link\"] = link[\"href\"]\n",
    "\n",
    "    # Extract rocket information\n",
    "    rocket_data = {}\n",
    "    rocket_section = soup.find(\"section\", class_=\"card section--center mdl-grid mdl-grid--no-spacing mdl-shadow--6dp\")\n",
    "    if rocket_section:\n",
    "        # Rocket Name\n",
    "        header_section = rocket_section.find(\"header\", class_=\"image_header\")\n",
    "        rocket_name_div = header_section.find(\"div\", class_=\"mdl-card__title-text\")\n",
    "        rocket_data[\"rocket_name\"] = rocket_name_div.find(\"span\").text.strip() if rocket_name_div else None\n",
    "\n",
    "        # Extract the rocket image\n",
    "        style_tags = soup.find(\"div\", class_=\"page-content\").find_all(\"style\")\n",
    "        if len(style_tags) > 1:\n",
    "            second_style = style_tags[1].string\n",
    "            if \"rocket_image\" in second_style:\n",
    "                match = re.search(r'background:\\s*url\\((.*?)\\)', second_style)\n",
    "                rocket_data[\"image_url\"] = match.group(1).strip(' \"\\'') if match else None\n",
    "\n",
    "        # Image Credit\n",
    "        image_credit_div = header_section.find(\"div\", class_=\"right_justified_text\")\n",
    "        rocket_data[\"image_credit\"] = image_credit_div.find(\"span\").text.strip() if image_credit_div else None\n",
    "\n",
    "        # Rocket details\n",
    "        rocket_details = rocket_section.find(\"div\", class_=\"mdl-card__supporting-text\")\n",
    "        if rocket_details:\n",
    "            rocket_details_info = rocket_details.find_all(\"div\", class_=\"mdl-cell mdl-cell--6-col-desktop mdl-cell--12-col-tablet\")\n",
    "            for detail in rocket_details_info:\n",
    "                key_value = detail.text.split(\":\")\n",
    "                if len(key_value) == 2:\n",
    "                    key = key_value[0].strip().replace(\" \", \"_\").lower()\n",
    "                    value = key_value[1].strip()\n",
    "                    rocket_data[key] = value\n",
    "\n",
    "        # Rocket details link\n",
    "        details_link_tag = rocket_section.find(\"div\", class_=\"mdl-card__actions mdl-card--border\").find(\"a\", class_=\"mdc-button\")\n",
    "        rocket_data[\"rocket_details_link\"] = details_link_tag.get(\"href\") if details_link_tag and details_link_tag.get(\"href\") else None\n",
    "\n",
    "        if rocket_data[\"rocket_details_link\"]:\n",
    "            rocket_data[\"rocket_id\"] = rocket_data[\"rocket_details_link\"].split(\"/\")[-1]\n",
    "\n",
    "    data[\"rocket_data\"] = rocket_data\n",
    "\n",
    "    # Extract mission details\n",
    "    mission_data = {}\n",
    "    mission_section = soup.find(\"section\", class_=\"section--center card white mdl-grid mdl-grid--no-spacing mdl-shadow--6dp\")\n",
    "    if mission_section:\n",
    "        mission_title = mission_section.find(\"h4\", class_=\"mdl-card__title-text\")\n",
    "        mission_data[\"title\"] = mission_title.text.strip() if mission_title else None\n",
    "\n",
    "        description_paragraph = mission_section.find(\"div\", class_=\"mdl-grid a\").find(\"p\")\n",
    "        mission_data[\"description\"] = description_paragraph.text.strip() if description_paragraph else None\n",
    "\n",
    "        details_grids = mission_section.find_all(\"div\", class_=\"mdl-grid a\")\n",
    "        if len(details_grids) > 1:\n",
    "            details_info = details_grids[1].find_all(\"div\", class_=\"mdl-cell\")\n",
    "            for detail in details_info:\n",
    "                text = detail.text.strip()\n",
    "                if \"Payloads:\" in text:\n",
    "                    mission_data[\"payloads\"] = text.split(\":\")[1].strip()\n",
    "                elif \"Total Mass:\" in text:\n",
    "                    mission_data[\"total_mass\"] = text.split(\":\")[1].strip()\n",
    "                elif \"Orbit\" in text:\n",
    "                    mission_data[\"orbit\"] = text\n",
    "\n",
    "    data[\"mission_data\"] = mission_data\n",
    "\n",
    "    # Extract location information\n",
    "    location_section = soup.find(\"h3\", string=\"Location\").find_next(\"section\", class_=\"card\")\n",
    "    if location_section:\n",
    "        location_title = location_section.find(\"h4\", class_=\"mdl-card__title-text\")\n",
    "        data[\"location\"] = location_title.text.strip() if location_title else None\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to scrape data and count successes and failures\n",
    "def scrape_with_counts(detail_urls, scrape_function):\n",
    "    data = []\n",
    "    failed_urls = []\n",
    "    success_counter, fail_counter = 0, 0\n",
    "\n",
    "    for url in detail_urls:\n",
    "        try:\n",
    "            result = scrape_function(url)\n",
    "            if result:\n",
    "                data.append(result)\n",
    "                success_counter += 1\n",
    "                print(f\"Scraped {url} (Total Successes: {success_counter})\")\n",
    "            else:\n",
    "                failed_urls.append(url)\n",
    "                fail_counter += 1\n",
    "                print(f\"Failed to scrape {url}. (Total Failures: {fail_counter})\")\n",
    "        except Exception as e:\n",
    "            failed_urls.append(url)\n",
    "            fail_counter += 1\n",
    "            print(f\"Error scraping {url}: {e} (Total Failures: {fail_counter})\")\n",
    "\n",
    "    return data, success_counter, fail_counter, failed_urls\n",
    "\n",
    "# Configurations for scraping past and upcoming launches\n",
    "configurations = {\n",
    "    \"past\": {\"base_url\": \"https://nextspaceflight.com/launches/past/\", \"pages\": 229, \"output\": \"past_launches.json\"},\n",
    "    \"upcoming\": {\"base_url\": \"https://nextspaceflight.com/launches/\", \"pages\": 15, \"output\": \"upcoming_launches.json\"}\n",
    "}\n",
    "\n",
    "# Iterate over configurations to scrape both past and upcoming launches\n",
    "for config_name, config in configurations.items():\n",
    "    print(f\"Scraping {config_name} launches...\")\n",
    "\n",
    "    # Create URL array and collect detail URLs\n",
    "    url_array = create_url_array(config[\"base_url\"], config[\"pages\"])\n",
    "    detail_urls = collect_detail_urls(url_array)\n",
    "\n",
    "    # Scrape launch data with counts\n",
    "    launch_data, total_success, total_failures, failed_urls = scrape_with_counts(detail_urls, scrape_launch_data)\n",
    "\n",
    "    # Save the scraped data to a JSON file\n",
    "    with open(config[\"output\"], 'w') as json_file:\n",
    "        json.dump(launch_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"{config_name.capitalize()} launch scraping complete.\")\n",
    "    print(f\"Total Success: {total_success}, Total Failures: {total_failures}. Data saved to '{config['output']}'.\")\n",
    "\n",
    "print(\"All scraping operations are complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
